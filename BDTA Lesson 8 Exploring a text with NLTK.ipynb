{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BDAT Lesson 8 - Exploring a Text with NLTK\n",
    "\n",
    "This notebook shows how you can explore aspects of a text using the Natural Langauge Took Kit (NLTK). It is based on [a similar one that is part of ALTA](https://github.com/sgsinclair/alta/blob/Rockwell's-Edits/ipynb/utilities/Exploring%20a%20text%20with%20NLTK.ipynb). Some of the things this hows you how to do include:\n",
    "\n",
    "* Tokenize a text \n",
    "* Generate a concordance for a word\n",
    "* Explore collocations (words that are located together)\n",
    "* Counding words and frequencies\n",
    "* Finding smiliar words and contexts\n",
    "\n",
    "For more on NLTK see the online version of the book [Natural Language Processing with Python](http://www.nltk.org/book/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Preparing for Exploration\n",
    "\n",
    "Before we can analyze a text we need to load it in and tokenize it. For tokenization and exploration we are going to use NLTK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Installing NTLK\n",
    "\n",
    "Before you can use NTLK you need to make sure it is installed. The [Anaconda Navigator](https://docs.continuum.io/anaconda/navigator) by default installs NLTK, but you can always test if it is installed by importing it with ```import nltk```. Try it. It will give you an error if you don't have it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Getting a Text\n",
    "\n",
    "Now we will get a text to process with NLTK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we see what text files we have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls *.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the \"Hume Enquiry.txt\" from the Gutenberg Project. You can use whatever text you want. We print the first 50 characters to check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theText2Use = \"Hume Enquiry.txt\"\n",
    "with open(theText2Use, \"r\") as fileToRead:\n",
    "    theString = fileToRead.read()\n",
    "    \n",
    "print(\"This string has\", len(theString), \"characters.\")\n",
    "print(theString[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**\n",
    "* Is this a good version of the text? How would we know?\n",
    "* How can we get rid of all the text at the start and end that are not part of the text?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Tokenization\n",
    "\n",
    "Now we tokenize the text using NTLK's tokenizer producing a list called \"listOfTokens\" and check the first words. Note that the NTLK tokenizer doesn't eliminate punctuation and doesn't lower case the words. You can tokenize using another method if you want. Then we create a NLTK text object from the tokens. Note how the text object behaves like a list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates a list of tokens from the text\n",
    "listOfTokens = nltk.word_tokenize(theString)\n",
    "\n",
    "# This creates a NLTK text object from a list of tokens\n",
    "theText = nltk.Text(listOfTokens)\n",
    "\n",
    "print(listOfTokens[:50]) # Show the first 50 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Concording\n",
    "\n",
    "Now we get a concordance for a word in one line. Note that we can control the width of the concordances. Edit the word to explore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theText.concordance(\"truth\", width=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that ```concordance``` is not case sensitive. This will give you a concordance of both capitalized and lower case words.\n",
    "\n",
    "If you want fewer or more lines then you need to add a parameter *lines=*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theText.concordance(\"the\", lines=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing that is annoying is that you can't easily save a concordance to a file and that is because the NLTK text object concordance is printed to the screen for exploration. You will need to cut and paste to a word processor to save this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Plot the Dispersion of Words\n",
    "\n",
    "We can easily plot the dispersion of words through the text. Note how it is case sensitive.\n",
    "\n",
    "The line ```%matplotlib inline``` makes sure that the plot is placed inline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "# This is to force the plots to show inline rather than in another window\n",
    "%matplotlib inline \n",
    "\n",
    "theText.dispersion_plot([\"truth\",\"Truth\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Counting Words and Frequencies\n",
    "\n",
    "You can also count words. This is case sensitive if you use the text object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(theText.count(\"truth\"), \" \", theText.count(\"Truth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it case insensitive we are going to lowercase every token and get a new list of tokens. We are also going to get rid of punctuation by keeping only the alphabetical tokens. Then we can count things in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theLowerTokens = []\n",
    "for token in listOfTokens:\n",
    "    if token.isalpha():\n",
    "        theLowerTokens.append(token.lower())\n",
    "    else:\n",
    "        theLowerTokens.append(token)\n",
    "\n",
    "print(theLowerTokens[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a more efficient way to do this using [list comprehension](http://python-3-patterns-idioms-test.readthedocs.io/en/latest/Comprehensions.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theLowerTokens = [token.lower() for token in listOfTokens if token[0].isalpha()]\n",
    "print(theLowerTokens[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfLowTokens.count(\"truth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With NLTK we can get word frequencies. These can be displayed as a table. We can then do other things with the frequency distribution object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theLowerFreqs = nltk.FreqDist(theLowerTokens)\n",
    "theLowerFreqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theLowerFreqs.tabulate(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theLowerFreqs[\"truth\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than get the count we can get the relative frequency which is the count divided by the number of tokens. This can be very useful for comparing across documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theLowerFreqs.freq(\"the\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Plot the Frequency of Words\n",
    "We can also plot the high frequency words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "theLowerFreqs.plot(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Plotting Content Words\n",
    "\n",
    "What if we want a to see just the high frequency content words. Here we get the NLTK English stopword list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "print(stopwords[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create a new list of tokens without the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theLowerContentWords = []\n",
    "\n",
    "for token in theLowerTokens:\n",
    "    if token not in stopwords:\n",
    "        theLowerContentWords.append(token)\n",
    "\n",
    "theLowerContentWords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theLowerContentWords = [token for token in theLowerTokens if token not in stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can a table of high frequency content words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theLowerContFreqs = nltk.FreqDist(theLowerContentWords)\n",
    "\n",
    "theLowerContFreqs.tabulate(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: How would you add the stopwords to get rid of words like \"may\", \"one\", \"must\", \"us\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we get the Frequency Distribution and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theLowerContFreqs.plot(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might also want to check how these words are used by looking at their concordance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theText.concordance(\"experience\", width=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Collocations, Similar Words, and Contexts\n",
    "\n",
    "### 4.1 Collocations\n",
    "\n",
    "NLTK will also let you explore co-locating words by which is meant sets of two or more words that appear frequently together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theText.collocations(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how we are getting a lot of bigrams with \"Gutenberg\". That's because NLTK looks for bigrams where the words appear together more often than alone. If you ask for more collocations you can see some that have to do with the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theText.collocations(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Similar Words\n",
    "\n",
    "We can get words that are **similar** to target words. These are not synonyms but words being used in similar contexts. You can use this to expland on a word you are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theText.similar(\"truth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use this to get concordances of sets of similar words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfWords2Conc = [\"reason\",\"fact\",\"knowledge\",\"ideas\"]\n",
    "for i in listOfWords2Conc:\n",
    "    print(i.upper() + \": \")\n",
    "    theText.concordance(i, width=80, lines=5)\n",
    "    print(\"--------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Common Contexts\n",
    "\n",
    "NLTK can give us common contexts for words that share them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theText.common_contexts([\"nature\", \"experience\"],10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Patterns\n",
    "\n",
    "We can use Regular Expressions on tokens with the ```findall``` method of the Text object. Some guidelines:\n",
    "\n",
    "* You are matching to tokens, not the raw text. The < and > indicate the token.\n",
    "* ```<.*>``` matches any token as ```.``` means any character and ```*``` means 0 or more of. ```?``` would mean \n",
    "* the paranthesis tell IPython what to show from the match. In first example below you can see how to show all the words right before the word you want.\n",
    "\n",
    "Here are some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theText.findall(\"(<.*>)<experience>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theText.findall(\"<.*><.*><nature>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theText.findall(\"(<.*><.*>)<truth>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theText.findall(\"<not><.*>?<true>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Homework: Exploring a Text\n",
    "Using the NLTK tools create a notebook that explores the text assembled of works by an author. Can you infer anything interested from the text. Explain what you find in the notebook.\n",
    "\n",
    "How would you check that your inferences are valid? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
